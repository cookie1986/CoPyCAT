import os
import re
import glob
import stanza
import pandas as pd

# download English model
stanza.download('en') 
# initialize English neural pipeline
nlp = stanza.Pipeline('en', processors='tokenize, pos, lemma')

# create empty list to store scores
outcome = []


# load each interview
for file in glob.glob('MT*.csv'):
    filename = file[-9:-4:]
    interview_ref = filename

    # read csv file
    tr = pd.read_csv(file, encoding='utf-8', usecols=['speaker','content'])

    # filter on turns generated by politicians
    pol_turns = tr[tr['speaker'] == 2]
    
    # get summation of interviewee responses
    pol_turns_global = '.'.join(x for x in pol_turns['content'].tolist())
    
    # remove punctuation
    pol_turns_global = re.sub(r"[^\w\d\s]"," ", pol_turns_global)
    
    # tokenize and lemmatize
    doc = nlp(pol_turns_global.lower())
    global_lemmas = [word.lemma for sent in doc.sentences for word in sent.words]    
    
    # unique tokens
    token_set = list(set(global_lemmas))
    
    # calculate type token ratio
    ttr = round(len(token_set)/len(global_lemmas)*100,2)

    # add to main scores
    outcome.append([filename, ttr])    
    
# load scores as a dataframe 
outcomeDF = pd.DataFrame(outcome, columns=['filename','score'])
