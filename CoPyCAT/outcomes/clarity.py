import os
import glob
import json
import stanza
import numpy as np
import pandas as pd

# download English model
stanza.download('en') 
# initialize English neural pipeline
nlp = stanza.Pipeline('en', processors='tokenize, pos, lemma')


# load concreteness dictionary
with open('') as t: 
    cncFile = t.read() 
    concreteness_dict = json.loads(cncFile)
    concreteness_dict =  {k.lower(): v for k, v in concreteness_dict.items()}
    
# create empty list to store scores
outcome = []

# internal function to perform Stanza lemmatization
def get_lemma(string):
    # load string as nlp object
    doc = nlp(string)
    # lemmatize
    lemma = [word.lemma for sent in doc.sentences for word in sent.words]
    return lemma

# internal function to get turn level cnc score
def get_cnc_values(tokens):
    cnc_vals = []
    
    for t in tokens:
        if t in concreteness_dict:
            cnc = concreteness_dict[t]
            cnc_vals.append(cnc)
    av_cnc = np.array(cnc_vals).mean()
    
    return av_cnc

# load each interview
for file in glob.glob('MT*.csv'):
    filename = file[-9:-4:]
    interview_ref = filename

    # read csv file
    tr = pd.read_csv(file, encoding='utf-8', usecols=['speaker','content'])

    # filter on turns generated by politicians
    pol_turns = tr[tr['speaker'] == 2]
    
    # lemmatize speech
    pol_turns['content'] = pol_turns['content'].apply(lambda x: list(set(get_lemma(x))))
    
    # get turn-level concreteness score
    pol_turns['cnc'] = pol_turns['content'].apply(lambda x: get_cnc_values(x))
    
    # take global mean of cnc scores
    global_cnc = round(pol_turns['cnc'].mean(),2)
    
    # add score to main list
    outcome.append([filename, global_cnc])
    
# load scores as a dataframe 
outcomeDF = pd.DataFrame(outcome, columns=['filename','score'])